===========================================================================================
15.10.2019

fias.institute
PW: BG7e61Whop69
Name: Nils Bertschinger

Probablity theory:
	uncertaintiy during measureings
	
	Random Variable: something that is uncertain
	Events: Probablity for a specific event (even number during die roll)
	
	sigma algebra A elem. sigma
		--> measureable events
		Sigma = sum of all measureable events
			if A,B elem. of Sigma --> what is the intersection event A n B

	Difference between continious and finate outcomes (sum of all events in cont. sigma does
		not work anymore)

	Probability measure P
	P * sigmar --> [0,1]

	P(empty Set) = 0
	P(omega) = 1 [any outcome occurse]
	A n B = empty Set --> P(A u B) = P(A) + P(B) if disjoint (indipendent?)
	P(omega\A) = 1 - P(A)
	
	Biased coin:
		omega = {0,1}, sigma = {empty set, {H}, {T}, {H,T} (either outcome)}
		P(empty set) = 0
		P({H,T}) = 1
		P({H}) = p
		P({T}) = 1 - p

		omega = {H,T,E} sigma = {empty set, {H}, {T}, {E}, {H,T}, {H,E}, {E,T}, {H,E,T}}
			--> with n variables we need n-1 variables to describe the etire system


	Only need to assign values to n-1 "atoms"


	Frequentist: measurement
		--> divide occurences of A, divide by number of trials
			if number of trials goes to infinity you converge to P(A)

	Beijen interpretation:
		Probablities are (subjective) believes
		closely linked to betting odds (can deal with single time experiments)
			(expl. weather exactly today in this moment, etc.)



	Joined probablities!!
		Things that depend on one another


	Example
	Temperature = {Lt, Ht}
	Humidity = {Lh, Hh}

	omega = omegat x omegah = {(Lt,Lh), (Ht, Lh), ...}
	

	P(temp elem. A X temp elem B) = sum(x elemA, y elem. B) P(t=x, h=y) (short: P(x,y)
		sum up probability of individual events

	marginal probablity: p(x) = P(temp = x x hum elem. omegah) = sum(y elem omegah) p(x,y)

	sum rule of probiblity;
	if only one event is interesting sum all of those events combined with omega of
	all other linked event

	conditional probablity
	p( p(At | Bh) = p(At x Bh) / p(Bh)

	notation:
	p(Lt | Hh) = P( {Lt} | {Hh})
	= ??? spoke to Martin


	Indipendece:
	p(x elem. A | y elem. B) = p(x elem A x y elem B)/ p(y elem B)
				 = p(x elem A) * p(y elem B) / p(y elem B)
				 = p(x elem A)

	Indipendence is cool, since smaler model
	Model that does not allow for dependencies is a useless model for machine learning


	machine learning: make simple models learn about dependencies in big Data sets
			try not to descripe all possible joins by assumeing indipendencies
			(make model small)


	Continous TV "omega elem. Real number R)
		--> no real probablity assignable to single outcomes

	Sigma alg. combined with borel Alg.
		--> {x} noelem. sigma

	work with intervals (a,b) elem sigma for all a < b elem R



	P(x elem (a,b)) = inteval(a,b) p(x) dx
	p(x) is probability density
	--> p(omega) --> R>=0
	
	P(x elem omega) = integral(omega) p(x) dx = 1
		(for us, probabilities and porbablity densities are non negative)
		(pobalilities cant be bigger then 1, probablility densities can be > 1, as long
		as sum over integral is 1)


	conditional desity
	p(x elem A | y elem B) = P(x elema x y elem B) / P(y elem B)

			       = integral(A) integral(B) p(xx,y) dx dy / integral(B) p(y) dy

	all axiomes above that work with probabilities also work with densities


	Normal distribution/ gaussian distribution
	p(x) = 1/ root(2pi sigma) * e^(-1/2 * (x- mue)^2 / sigma^2)


 	
